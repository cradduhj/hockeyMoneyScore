---
title: "SLM 418 Final"
author: "Harrison Cradduck"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(rvest)
library(dplyr)
library(stringr)

# List of game URLs for 3 specific games
urls <- c(
  "https://www.hockey-reference.com/boxscores/202310140EDM.html",
  "https://www.hockey-reference.com/boxscores/202310210EDM.html",
  "https://www.hockey-reference.com/boxscores/202310260EDM.html"
)

# Create an empty list to store all the data frames
all_data <- list()

# Loop through each game URL to extract the data
for (url in urls) {
  
  # Extract date and team from the URL
  date <- str_extract(url, "\\d{8}")  # YYYYMMDD format
  team <- str_extract(url, "(?<=\\d{8})[A-Za-z]{3}")
  
  # Read the webpage and extract the tables
  page <- read_html(url)
  tables <- page %>% html_nodes("table")
  
  # Read table 3 and table 7, clean them
  table_5 <- html_table(tables[[5]], fill = TRUE) %>% set_names(.[1, ]) %>% tail(-1) %>% head(-1)
  table_5 <- table_5[, -1]
  
  table_14 <- html_table(tables[[14]], fill = TRUE) %>% head(-1)
  
  # Merge table 3 and table 7 based on the 'Player' column
  merged_5_14 <- merge(table_5, table_14, by = "Player")
  
  # Add Date and Team columns after the merge
  merged_5_14$Date <- date
  merged_5_14$Team <- team
  
  # Add the data to the list
  all_data[[length(all_data) + 1]] <- merged_5_14
}

# Combine all data frames into one
final_data <- bind_rows(all_data)

# Print the final combined dataframe
print(final_data)
```

```{r}
library(rvest)
library(dplyr)
library(stringr)

# Generate the list of all game dates in 2023-2024 (October 2023 to April 2024)
dates_2023_2024 <- seq.Date(from = as.Date("2023-10-01"), to = as.Date("2024-04-30"), by = "day")

# Filter the dates for when Edmonton Oilers played (EDM team)
# Assuming their games are posted on Hockey Reference as "YYYYMMDD"EDM
urls <- paste0("https://www.hockey-reference.com/boxscores/", format(dates_2023_2024, "%Y%m%d"), "EDM.html")

# Create an empty list to store all the data frames
all_data <- list()

# Loop through each game URL to extract the data
for (url in urls) {
  
  # Extract date and team from the URL
  date <- str_extract(url, "\\d{8}")  # YYYYMMDD format
  team <- str_extract(url, "(?<=\\d{8})[A-Za-z]{3}")
  
  # Read the webpage and extract the tables
  page <- tryCatch(read_html(url), error = function(e) NULL)
  
  # Skip if the page doesn't exist or there was an error
  if (is.null(page)) {
    next
  }
  
  # Extract all tables on the page
  tables <- page %>% html_nodes("table")
  
  # Check that table 5 and table 14 exist on the page
  if (length(tables) >= 14) {
    # Read table 5 and table 14, clean them
    table_5 <- html_table(tables[[5]], fill = TRUE) %>% set_names(.[1, ]) %>% tail(-1) %>% head(-1)
    table_5 <- table_5[, -1]
    
    table_14 <- html_table(tables[[14]], fill = TRUE) %>% head(-1)
    
    # Merge table 5 and table 14 based on the 'Player' column
    merged_5_14 <- merge(table_5, table_14, by = "Player")
    
    # Add Date and Team columns after the merge
    merged_5_14$Date <- date
    merged_5_14$Team <- team
    
    # Add the data to the list
    all_data[[length(all_data) + 1]] <- merged_5_14
  }
}

# Combine all data frames into one
final_data <- bind_rows(all_data)

# Print the final combined dataframe
print(final_data)
```



